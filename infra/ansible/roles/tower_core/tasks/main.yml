# -----------------------------------------------------------------------------
# [FILE] infra/ansible/roles/tower_core/tasks/main.yml
# [DESCRIPTION] Main provisioning logic for the Central Aggregation Node (Tower)
# [VERSION] v1.5.0 (Grid Health Dashboard Integration)
# [DATE] 2026-01-14
# [AUTHOR] ADSB Research Grid Automation Team
# -----------------------------------------------------------------------------
# [CHANGELOG]
# v1.5.0: Integrated 'Grid Health Center' dashboard for OpenSearch log analytics.
# v1.4.1: Adjusted ingestion schedule to xx:30 to mitigate zero-minute congestion.
# v1.4.0: Implemented Root-level execution for ingestion to resolve permission faults.
# v1.3.1: Added Dual-Dashboard deployment (Chase/Overview) for Owntracks.
# -----------------------------------------------------------------------------

# ==============================================================================
# PHASE 1: SYSTEM PRE-REQUISITES
# ==============================================================================
# [RATIONALE] The Tower Core requires specific kernel parameters for ElasticSearch/OpenSearch
# and standard system utilities for monitoring (htop/iotop) and data processing (jq).

- name: "[System] Install core dependencies"
  apt:
    name:
      - curl
      - git
      - htop
      - iotop       # I/O monitoring for NVMe throughput analysis
      - jq          # JSON processor for parsing API responses
      - rsyslog     # CRITICAL: Required for centralized logging aggregation
    state: present
    update_cache: yes
  tags: [system, common]

- name: "[System] Future-Proofing Kernel (Prepare for OpenSearch)"
  # OpenSearch uses mmapfs to store indices. The default OS limit is too low.
  sysctl:
    name: vm.max_map_count
    value: '262144'
    state: present
    reload: yes
  tags: [system, kernel]

# ==============================================================================
# PHASE 2: DOCKER & USER PERMISSIONS
# ==============================================================================
# [RATIONALE] The container stack runs under the 'docker' group. The administrative
# user must be added to this group to manage the stack without sudo escalation.

- name: "[Docker] Ensure Docker service is active"
  service:
    name: docker
    state: started
    enabled: yes
  tags: [docker]

- name: "[Users] Add admin to docker group"
  user:
    name: "{{ ansible_user }}"
    groups: docker
    append: yes
  tags: [user, docker]

# ==============================================================================
# PHASE 3: STORAGE HIERARCHY
# ==============================================================================
# [RATIONALE] Persistent storage is mapped to host directories to ensure data
# survival across container rebuilds. Structure follows the FHS where applicable.

- name: "[Storage] Create service volume directories"
  file:
    path: "{{ item }}"
    state: directory
    mode: '0755'
    owner: "{{ ansible_user }}"
    group: "docker"
  loop:
    - /opt/tower
    - /opt/tower/mosquitto/config
    - /opt/tower/mosquitto/data
    - /opt/tower/mosquitto/log
    - /opt/tower/telegraf
    - /opt/tower/syslog_data
    - /opt/tower/syslog_config
    - /opt/tower/influxdb
    - /opt/tower/grafana
  tags: [storage, setup]

- name: "[Grafana] Create provisioning directories"
  # Structured provisioning allows "Infrastructure as Code" management of Dashboards.
  file:
    path: "{{ item }}"
    state: directory
    mode: '0755'
    owner: "{{ ansible_user }}"
    group: "docker"
  loop:
    - /opt/tower/grafana/provisioning
    - /opt/tower/grafana/provisioning/dashboards
    - /opt/tower/grafana/provisioning/datasources
  tags: [grafana, storage]

# ==============================================================================
# PHASE 4: CONFIGURATION INJECTION
# ==============================================================================
# [RATIONALE] Configuration files are templated (Jinja2) to inject environment-specific
# variables (IPs, credentials) at runtime, preventing hardcoded secrets.

- name: "[Config] Deploy Mosquitto Config"
  template:
    src: mosquitto/mosquitto.conf.j2
    dest: /opt/tower/mosquitto/config/mosquitto.conf
    mode: '0644'
  tags: [mosquitto, config]

- name: "[Config] Deploy Telegraf Config"
  template:
    src: telegraf.conf.j2
    dest: /opt/tower/telegraf/telegraf.conf
    mode: '0644'
  tags: [telegraf, config]

- name: "[Grafana] Deploy Datasource Provisioning"
  template:
    src: grafana/provisioning/datasources/datasource.yml.j2
    dest: /opt/tower/grafana/provisioning/datasources/datasource.yml
    mode: '0644'
  tags: [grafana, config]

- name: "[Grafana] Deploy Dashboard Provisioning"
  template:
    src: grafana/provisioning/dashboards/dashboard.yml.j2
    dest: /opt/tower/grafana/provisioning/dashboards/dashboard.yml
    mode: '0644'
  tags: [grafana, config]

- name: "[Grafana] Deploy System Overview Dashboard JSON"
  template:
    src: grafana/provisioning/dashboards/system_overview.json.j2
    dest: /opt/tower/grafana/provisioning/dashboards/system_overview.json
    mode: '0644'
  tags: [grafana, dashboard]

- name: "[Grafana] Deploy Analytical Dashboards"
  # Consolidated deployment loop for all operational dashboards.
  # - Chase: Real-time (1s refresh) for active tracking.
  # - Overview: Historical (1m refresh) for pattern analysis.
  # - Grid Health: Log analytics (OpenSearch) for system debugging.
  template:
    src: "{{ item.src }}"
    dest: "/opt/tower/grafana/provisioning/dashboards/{{ item.dest }}"
    owner: "{{ ansible_user }}"
    group: "docker"
    mode: "0644"
  loop:
    - { src: "grafana/provisioning/dashboards/owntracks_chase.json.j2", dest: "owntracks_chase.json" }
    - { src: "grafana/provisioning/dashboards/owntracks_overview.json.j2", dest: "owntracks_overview.json" }
    # v1.5.0: Added Grid Health Center
    - { src: "grafana/provisioning/dashboards/grid_health.json.j2", dest: "grid_health.json" }
  tags: [grafana, dashboard]

# ==============================================================================
# PHASE 5: SYSLOG SERVER (Aggregator)
# ==============================================================================
# [RATIONALE] The Tower acts as the centralized log sink. We enable both UDP (fast, standard)
# and TCP (reliable) listeners on port 514 to accommodate different network conditions.

- name: "[Syslog] Enable UDP and TCP Reception (Port 514)"
  ansible.builtin.lineinfile:
    path: /etc/rsyslog.conf
    regexp: '{{ item.regexp }}'
    line: '{{ item.line }}'
    state: present
  loop:
    # Enable UDP (Standard)
    - { regexp: '^#module\(load="imudp"\)', line: 'module(load="imudp")' }
    - { regexp: '^#input\(type="imudp"',    line: 'input(type="imudp" port="514")' }
    # Enable TCP (Reliable - For Distant Sensors)
    - { regexp: '^#module\(load="imtcp"\)', line: 'module(load="imtcp")' }
    - { regexp: '^#input\(type="imtcp"',    line: 'input(type="imtcp" port="514")' }
  become: yes
  register: rsyslog_config
  tags: [syslog, config]

- name: "[Syslog] Create dedicated remote log rule"
  # This rule separates logs by Source IP, preventing the "Needle in a Haystack" problem
  # when debugging a specific sensor node.
  copy:
    dest: /etc/rsyslog.d/10-remote.conf
    content: |
      # Save remote logs to a separate file based on IP ADDRESS (Safer)
      $template RemoteLogs,"/var/log/remote-%FROMHOST-IP%.log"
      
      # If the message is NOT from localhost, use the RemoteLogs template
      if ($fromhost-ip != "127.0.0.1") then ?RemoteLogs
      & stop
    mode: '0644'
  become: yes
  notify: Restart Syslog
  tags: [syslog, config]

- name: "[Syslog] Restart Rsyslog to apply changes"
  service:
    name: rsyslog
    state: restarted
  become: yes
  when: rsyslog_config.changed
  tags: [syslog]

# ==============================================================================
# PHASE 6: FIREWALL SECURITY (UFW)
# ==============================================================================
# [RATIONALE] We explicitly allow only the necessary ingestion ports.
# Port 1883: MQTT (Telemetry) | Port 514: Syslog (Logs)

- name: "[Firewall] Allow MQTT (1883) for Autel/Sensors"
  community.general.ufw:
    rule: allow
    port: '1883'
    proto: tcp
  become: yes
  tags: [firewall, security]

- name: "[Firewall] Allow Syslog UDP (Standard)"
  community.general.ufw:
    rule: allow
    port: '514'
    proto: udp
  become: yes
  tags: [firewall, security]

- name: "[Firewall] Allow Syslog TCP (Reliable Delivery)"
  community.general.ufw:
    rule: allow
    port: '514'
    proto: tcp
  become: yes
  tags: [firewall, security]

# ==============================================================================
# PHASE 7: STACK DEPLOYMENT (Docker Compose)
# ==============================================================================
# [RATIONALE] The core visualization stack (TIG) is deployed via Docker Compose
# for modularity and ease of updates.

- name: "[Stack] Deploy Docker Compose Manifest"
  template:
    src: docker-compose.yml.j2
    dest: /opt/tower/docker-compose.yml
    mode: '0644'
  tags: [stack, docker]

- name: "[Launch] Spin up Tower Stack (TIG + MQTT + Syslog)"
  community.docker.docker_compose_v2:
    project_src: /opt/tower
    state: present
    pull: missing
  tags: [stack, docker]

# ==============================================================================
# PHASE 8: ADVANCED ANALYTICS (OpenSearch)
# ==============================================================================
- import_tasks: opensearch.yml
  tags: [opensearch, arkime]

# ==============================================================================
# PHASE 9: DATA COLLECTOR & AUTOMATION
# ==============================================================================
# [RATIONALE] This section manages the scientific data ingestion pipeline.
# It ensures scripts are deployed and scheduled to run reliably.

- name: "[Storage] Create Data Directories"
  file:
    path: "{{ item }}"
    state: directory
    owner: admin
    group: admin
    mode: '0755'
  loop:
    - /opt/adsb-grid
    - /opt/adsb-grid/data
    - /opt/adsb-grid/scripts
  tags: [storage, collector]

- name: "[Collector] Deploy Fetch Script"
  template:
    src: scripts/fetch_logs.sh.j2
    dest: /opt/adsb-grid/scripts/fetch_logs.sh
    owner: admin
    group: admin
    mode: '0755'
  tags: [collector, scripts]

# ------------------------------------------------------------------------------
# [CRON] AUTOMATED INGESTION SCHEDULING
# ------------------------------------------------------------------------------
# NOTE: We shifted the hourly schedule to xx:30 to avoid network congestion at the
# top of the hour. We also enforce 'root' execution to ensure write permissions
# to the /var/log/ directory.

- name: "[Cron] REMOVE Legacy Admin Job (Prevent Duplicate Run)"
  # CRITICAL FIX: Removes the old job running as 'admin' which caused race conditions.
  cron:
    name: "ADSB Hourly Log Sync"
    user: admin
    state: absent
  tags: [cron, cleanup]

- name: "[Cron] Schedule Hourly Science Data Sync (Root)"
  # Runs at 30 minutes past every hour.
  cron:
    name: "ADSB Hourly Log Sync"
    minute: "5"
    hour: "*"
    user: root
    job: "/opt/adsb-grid/scripts/fetch_logs.sh >> /var/log/adsb-fetch.log 2>&1"
    state: present
  tags: [cron, collector]

- name: "[Cron] Schedule Daily Full Backup (Root)"
  # Runs once a day at 04:00 AM.
  cron:
    name: "ADSB Daily Log Collection"
    minute: "0"
    hour: "4"
    user: root
    job: "/opt/adsb-grid/scripts/fetch_logs.sh"
    state: present
  tags: [cron, collector]

- import_tasks: logging.yml
