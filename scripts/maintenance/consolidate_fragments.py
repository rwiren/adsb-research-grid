#!/usr/bin/env python3
# ------------------------------------------------------------------
# [FILE] scripts/maintenance/consolidate_fragments.py
# [AUTHOR] Richard Wir√©n
# [DATE] 2026-01-12
# [VERSION] 1.1.0
# [DESCRIPTION] 
#   Maintenance utility for the ADS-B Research Data Lake.
#   It scans for fragmented sensor logs (e.g., 1-minute interval files
#   generated by unstable services), merges them into a single 
#   daily compressed CSV, and removes the fragments to reduce inode usage.
# ------------------------------------------------------------------

import os
import glob
import pandas as pd
from datetime import datetime
import re
import sys

# ------------------------------------------------------------------
# CONFIGURATION
# ------------------------------------------------------------------
DATA_ROOT = "research_data"

# Regex to identify fragment files.
# Expected format: name_YYYY-MM-DD_HHMM.csv.gz
# Captures group 1: Base Name (e.g., sensor-west_aircraft_log)
# Captures group 2: Date (e.g., 2026-01-12)
FRAGMENT_PATTERN = r"(.*)_(\d{4}-\d{2}-\d{2})_\d{4}\.csv\.gz$"

def consolidate_data():
    start_time = datetime.now()
    print(f"[{start_time.strftime('%H:%M:%S')}] üßπ Starting Data Consolidation...")
    
    merged_count = 0
    
    # Walk through all directories in research_data
    for root, dirs, files in os.walk(DATA_ROOT):
        # Dictionary to group fragments: { 'unique_file_key': [list_of_paths] }
        groups = {}
        
        for file in files:
            match = re.match(FRAGMENT_PATTERN, file)
            if match:
                base_name = match.group(1) 
                date_part = match.group(2) 
                
                # Create a unique key for the day (e.g., "sensor-west_aircraft_log_2026-01-12")
                key = f"{base_name}_{date_part}"
                
                if key not in groups:
                    groups[key] = []
                groups[key].append(os.path.join(root, file))

        # Process each group found in this directory
        for key, fragment_paths in groups.items():
            # Only merge if we actually have multiple fragments
            if len(fragment_paths) > 1:
                target_filename = f"{key}.csv.gz"
                target_path = os.path.join(root, target_filename)
                
                print(f"   ‚îî‚îÄ‚îÄ üì¶ Merging {len(fragment_paths)} fragments -> {target_filename}")
                
                try:
                    # [DATA SCIENCE] Read all fragments into a list of DataFrames
                    # Force dtype=str to prevent type inference errors during merge
                    df_list = []
                    for fp in sorted(fragment_paths):
                        try:
                            df_temp = pd.read_csv(fp, dtype=str, compression='gzip')
                            df_list.append(df_temp)
                        except pd.errors.EmptyDataError:
                            print(f"       ‚ö†Ô∏è  Empty fragment ignored: {os.path.basename(fp)}")
                            pass 

                    if df_list:
                        # Concatenate
                        combined_df = pd.concat(df_list, ignore_index=True)
                        
                        # [DEVOPS] Write atomically (sort of) - overwrite target
                        combined_df.to_csv(target_path, index=False, compression='gzip')
                        
                        # Verify the file exists and has size before deleting source files
                        if os.path.exists(target_path) and os.path.getsize(target_path) > 0:
                            for fp in fragment_paths:
                                os.remove(fp)
                            merged_count += 1
                        else:
                            print(f"       ‚ùå Merge failed (Target empty/missing). Keeping fragments.")
                            
                except Exception as e:
                    print(f"       ‚ùå Critical Error merging {key}: {e}")

    duration = datetime.now() - start_time
    print(f"[{datetime.now().strftime('%H:%M:%S')}] ‚úÖ Consolidation Complete. Merged {merged_count} daily sets in {duration}.")

if __name__ == "__main__":
    consolidate_data()
